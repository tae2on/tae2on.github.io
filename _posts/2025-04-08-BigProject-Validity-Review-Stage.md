--- 
title: "빅프로젝트 | 타당성 검토 단계" 
date: 2025-04-08 17:30:45 +0900
achieved: 2025-01-24 18:00:2 +0900
math: true
categories: [Bootcamp, KT Aivle School]
tags: [Bootcamp, KT Aivle School, Big Project]
---
---------- 	
> KT 에이블스쿨 빅프로젝트 과제 심의 단계(1.13 ~ 1.24)에 대한 글입니다. 
{: .prompt-info } 

## **빅프로젝트 개요**
빅프로젝트는 기업의 실무 프로세스 절차와 방식을 경험(고객정의, 이슈 파악, 솔루션 제안 및 구현)하며 문제 해결 역량과 협업 능력을 키우는 실무형 프로젝트입니다. 이를 통해 AI/IT 기술을 활용한 디지털 혁신 솔루션을 제안하고 구현하는 과정이 포함됩니다. 

### **빅프로젝트 일정**

- 1 ~ 2주차: 과제 심의 단계
- 3 ~ 4주차: 타당성 검토 단계 
- 5 ~ 6주차: 품질 평가 단계
- 7주차: 완료 처리 단계 
- 8주차: 발표회 

## **선정된 주제**

**도서 내용을 웹툰화를 통한 광고 제작**

1. 도서 내용의 서문을 자연어 처리 모델을 통해 스크립트 형식으로 변형
2. 생성된 스크립트 내용을 통해 웹툰 형식의 이미지 생성
3. 완성된 웹툰 이미지를 광고 및 마케팅 자료로 활용하여 도서의 홍보 효과 극대화

## **팀원 소개**
진현(팀장), 이상화, 이성훈, 이현제, 정민규, 황태언

### **역할 분담**

- Web: 진현, 이성훈
- GenAI: 이현제, 정민규
- NLP: 이상화, 황태언

도서 내용을 웹툰화하기 위해서는 요약이 아닌 등장인물의 특징 및 시대적 배경, 스토리텔링 중심의 스크립트 형식으로 필요하다고 생각하였습니다. 원작의 분위기를 유지하면서 장면 간의 전환이 매끄럽게 이어지는 스토리 구성이 중요하다고 생각하여 해당 역할을 NLP 팀이 담당하였습니다. 

## **데이터셋 수집**
도서 내용을 기반으로 한 웹툰 형식의 이미지 제작을 위해 텍스트 기반 원문 데이터를 수집하였습니다. 

> ※ 수집한 텍스트는 분석과 실험을 위한 학습 용도로만 사용하였으며, 외부 공개나 상업적 사용은 포함하지 않았습니다.

### **데이터 전처리**
도서의 앞부분 내용을 웹툰으로 제작하여 독자의 흥미를 유도하고 최종적으로 도서 구매로 이어지게 하는 것이 목적이므로 도서의 앞부분 약 2만 자 분량만을 발췌하여 데이터로 활용하였습니다.
이 텍스트를 기반으로 등장인물, 대화, 상황 설명 등을 포함한 스토리라인을 구성하기 위해 Open Ai를 활용하여 컷 단위로 변환된 학습용 정답 데이터를 생성하였습니다.

### **데이터셋 구조**
학습용 정답 데이터는 한 컷 단위로 장면 설명(caption)과 대화(dialogues)를 함께 포함하는 JSON 형식으로 구성하였습니다. 
- `location`: 장면이 발생한 장소
- `caption`: 이미지 생성 프롬프트에 들어갈 장면 설명
- `dialogues`: 등장인물의 대화 목록
    - `speaker`: 말하는 인물 이름 
    - `dialogue`: 인물의 발화 내용

이 구조를 기반으로 모델 학습을 진행하였습니다.

## **시나리오 모델**

### **1차 시도**

- 입력 기준: 도서 서문의 약 2만 자 분량 전체
- 활용 모델: `kobart`, `koT5`, `kogpt2`
    - 세 모델을 비교 실험 후, 가장 성능이 우수한 모델을 선정하여 파인튜닝 예정
- 목표 출력: 30~40컷 분량의 웹툰용 장면 및 대사 구성
 
**문제점 및 한계**
- 최대 토큰이 1024로 한정되어 있어서 텍스트 전체를 한번에 입력하여 처리하는 것에 한계가 있음
    - 모델의 입력 범위 제약으로 인해 그에 대한 결과가 확인 불가 

### **2차 시도**

- 개선 방안
    - 책의 서문 2만자를 500자 단위로 분할
    - 보다 짧은 단위에서 연속적인 스토리 구성 시도
- 활용 모델: `KoBART`, `KoT5`, `KoGPT2`
    - 세 모델을 비교 실험 후, 가장 성능이 우수한 모델을 선정하여 파인튜닝 예정

**활용 모델의 결과**
1. `KoBART` 사용 결과 
    - 시나리오 형태로 장면 구성 
    - 스토리텔링 구조로 재구성하는 데 강점이 보임
2. `KoT5` 사용 결과
    - 특정 문장을 의미 없이 여러번 반복 생성
3. `KoGPT2` 사용 결과 
    - 입력을 기반으로 새로운 문장을 생성하여 도서 내용의 의미 유지 및 요약 목적에는 부적합 


→ 세 모델 사용 결과를 반영하여 `koBART`를 사용하는 것이 제일 적합하다고 판단하였습니다. 

**문제점 및 한계**
- 일부 데이터는 2만자 기준에서 문장이 안 끊겼는데도 분할된 경우 존재
- 장면의 전환이나 이야기의 흐름이 부자연스럽고 불완전
    - 중요한 문맥 생략으로 인해 몰입도 낮음
    - 컷 간의 연결성

#### **BART**
![BART](https://github.com/tae2on/tae2on.github.io/blob/main/assets/img/kobart.png?raw=true)

`BART`(Bidirectional and Auto-Regressive Transformers): BERT와 GPT 모델의 구조적 특징을 결합한 모델로 일반적인 seq2seq 아키텍처 골격을 가집니다.

- 모델 타입
    - `Transformer` 기반의 `Encoder-Decoder` 구조
        - `Encoder`: 입력 문장을 이해하고 압축된 의미 표현을 생성
        - `Decoder`: 표현을 바탕으로 새로운 문장(요약, 번역, 생성 등)을 만들어냄
- 주요 용도
    - 텍스트 요약, 생성, 번역, 스토리 재구성 등 다양하게 사용
- 입력 및 출력의 최대 토큰
    - 최대 입력 길이: 1024개
    - 최대 출력 길이: 1024개

## **인물 특징 및 시대적 배경 추출 모델**
### **1차 시도**
`NER`(개체명 인식) 모델은 사람, 장소, 조직, 날짜 등 의미 있는 고유명사를 식별하는 모델로 등장인물의 이름이나 지명, 시대적 배경과 관련된 단어를 추출하는데 활용하는 수 있을 거라고 생각하였습니다. 하지만 도서 텍스트를 이용하여 테스트해 본 결과, 이름 혹은 시대적 배경 등을 제대로 추출하지 못하는 것을 확인하였습니다. 

## **고찰**
초기에는 긴 텍스트를 입력하면 자연스러운 시나리오가 생성될 것이라고 기대했지만 입력 토큰 제한으로 인해 원하는 결과를 얻지 못했습니다. 이후 모델의 구조와 작동 원리에 대해 학습한 내용을 반영하여 시도한 결과, 나은 결과를 얻을 수 있었습니다. 이 과정을 통해 모델을 단순히 적용하는 것보단 모델 구조와 모델의 한계를 이해하고 이에 맞게 전략을 조정하는 과정이 중요하다는 것을 깨달을 수 있었습니다. 